{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNLd0SSAajNGz2Mu0UNgra5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UGUcyJJWyQ3-","executionInfo":{"status":"ok","timestamp":1689792997714,"user_tz":-120,"elapsed":13512,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"outputId":"cf9cd22d-e87d-4939-c3ac-7ced75ab6803"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scenedetect[opencv]\n","  Downloading scenedetect-0.6.1-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Click in /usr/local/lib/python3.10/dist-packages (from scenedetect[opencv]) (8.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scenedetect[opencv]) (1.22.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scenedetect[opencv]) (4.65.0)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from scenedetect[opencv]) (1.4.4)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from scenedetect[opencv]) (4.7.0.72)\n","Installing collected packages: scenedetect\n","Successfully installed scenedetect-0.6.1\n"]}],"source":[" ### Installations ###\n","#####################\n","\n","# scene detection\n","!pip install --upgrade scenedetect[opencv]"]},{"cell_type":"code","source":["### Imports ###\n","###############\n","\n","# general\n","import numpy as np\n","import pandas as pd\n","import shutil\n","import os\n","from tqdm import tqdm\n","\n","# image processing\n","import cv2\n","from google.colab.patches import cv2_imshow\n","\n","# scene detection\n","from scenedetect import detect, ContentDetector\n","\n","# audio processing\n","import librosa"],"metadata":{"id":"5Dp6ZU7YyYi1","executionInfo":{"status":"ok","timestamp":1689793001256,"user_tz":-120,"elapsed":3165,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["### Set data directory\n","##################\n","\n","# connect to drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# set data directory\n","data_dir = '/content/drive/MyDrive/0_Masterarbeit/2_Pipelines/Data'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J-FXFlEPzjdw","executionInfo":{"status":"ok","timestamp":1689793020887,"user_tz":-120,"elapsed":19639,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"outputId":"06d17955-70e4-42e0-ce25-a9d87e9c10bc"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["### Upload video and audio files ###\n","####################################\n","\n","data_file = 'ferrari'\n","\n","# copy zip files\n","shutil.copy(os.path.join(data_dir, f'Video_{data_file}.zip'), './')\n","shutil.copy(os.path.join(data_dir, f'Audio_{data_file}.zip'), './')\n","\n","# create folders to unpack zip files to\n","os.makedirs('./Video')\n","os.makedirs('./Audio')\n","\n","# unpack zip files\n","shutil.unpack_archive(f'./Video_{data_file}.zip', extract_dir = './Video')\n","shutil.unpack_archive(f'./Audio_{data_file}.zip', extract_dir = './Audio')"],"metadata":{"id":"7U_C0dBommC6","executionInfo":{"status":"ok","timestamp":1689793071846,"user_tz":-120,"elapsed":50963,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### Extract Color Values"],"metadata":{"id":"yDapM06AMiKJ"}},{"cell_type":"code","source":["### Function to extract color values ###\n","########################################\n","\n","def cal_color_values(video, n_parts = 3):\n","\n","  # Open the video file\n","  cap = cv2.VideoCapture(video)\n","\n","  # Get the number of frames and frame rate\n","  num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","  fps = int(cap.get(cv2.CAP_PROP_FPS))\n","\n","  # Define proportion of frames that are considered (in this case: one per second)\n","  interval = fps\n","\n","  # Calculate the number of frames in each part\n","  frames_per_part = int(num_frames / n_parts)\n","\n","  # Initialize a list to store the average hue of each part\n","  average_hue = []\n","  average_sat = []\n","  average_value = []\n","\n","\n","  ### Loop to consider each defined part seperately ###\n","  #####################################################\n","  for i in range(n_parts):\n","\n","    # Set the starting frame for the part\n","    start_frame = i * frames_per_part\n","\n","    # Set the position of the video file to the starting frame\n","    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n","\n","    # Initialize the running total mean for the channels and counter for the number of considered frames for the part\n","    total_hue = 0\n","    total_sat = 0\n","    total_value = 0\n","    num_fpp_considered = 0\n","\n","    ### Loop over the frames in the part ###\n","    ########################################\n","    for j in range(frames_per_part):\n","\n","        # Read the frame\n","        ret, frame = cap.read()\n","\n","        # Break if no more frames\n","        if not ret:\n","            break\n","\n","        # Compute if first frame of interval\n","        if j%interval == 0:\n","\n","            # Convert the frame to the HSV color space\n","            hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n","\n","            # Extract the hsv channels\n","            hue_channel = hsv_frame[:, :, 0] # color information\n","            sat_channel = hsv_frame[:, :, 1] # saturation/ intensity/purity\n","            value_channel = hsv_frame[:, :, 2] # brightness/lightness\n","\n","            # Calculate mean of channels over considered frame\n","            mean_hue = hue_channel.mean()\n","            mean_sat = sat_channel.mean()\n","            mean_value = value_channel.mean()\n","\n","            # Add to running total mean for each channel\n","            total_hue += mean_hue\n","            total_sat += mean_sat\n","            total_value += mean_value\n","\n","            # Count considered frames for part\n","            num_fpp_considered  += 1\n","\n","\n","    # Calculate the averages part and add to list\n","    average_hue.append(total_hue/num_fpp_considered)\n","    average_sat.append(total_sat/num_fpp_considered)\n","    average_value.append(total_value/num_fpp_considered)\n","\n","  # Release the video capture object and close the windows\n","  cap.release()\n","  cv2.destroyAllWindows()\n","\n","  #return np.array(average_hue), np.array(average_sat), np.array(average_value)\n","  return average_hue, average_sat, average_value"],"metadata":{"id":"l7Q9XE_YykZS","executionInfo":{"status":"ok","timestamp":1689793071847,"user_tz":-120,"elapsed":11,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["### Apply function to all videos ###\n","###################################\n","\n","# Create empty lists to store\n","video_id_list = []\n","hue_list = []\n","sat_list = []\n","value_list = []\n","\n","# Loop through videos and extract color values\n","n_parts = 3\n","for video_file in tqdm(os.listdir('./Video')):\n","  average_hue, average_sat, average_value = cal_color_values(os.path.join('./Video', video_file), n_parts = n_parts)\n","\n","  video_id_list.append(video_file[:-4])\n","  hue_list.append(average_hue)\n","  sat_list.append(average_sat)\n","  value_list.append(average_value)"],"metadata":{"id":"tXkFsi8s9lGZ","executionInfo":{"status":"ok","timestamp":1689793745036,"user_tz":-120,"elapsed":673198,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3bae0848-d6a9-4da5-f2a5-a89b0c11bce1"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 182/182 [11:13<00:00,  3.70s/it]\n"]}]},{"cell_type":"code","source":["### Create pandas dataframe ###\n","###############################\n","\n","# Normalize values according to opencv documentation:\n","# -> https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_colorspaces/py_colorspaces.html\n","\n","# video id df\n","video_id_df = pd.Series(video_id_list, name = 'video_id')\n","\n","# create hue df\n","hue_df = pd.DataFrame(np.array(hue_list) / 179)\n","hue_name_dict = {i:f\"hue_part_{i+1}\" for i in range(n_parts)}\n","hue_df = hue_df.rename(columns = hue_name_dict)\n","\n","# create sat df\n","sat_df = pd.DataFrame(np.array(sat_list) / 255)\n","sat_name_dict = {i:f\"sat_part_{i+1}\" for i in range(n_parts)}\n","sat_df = sat_df.rename(columns = sat_name_dict)\n","\n","# create value df\n","value_df = pd.DataFrame(np.array(value_list) / 255)\n","value_name_dict = {i:f\"value_part_{i+1}\" for i in range(n_parts)}\n","value_df = value_df.rename(columns = value_name_dict)\n","\n","# merge into one df\n","color_df = pd.concat([video_id_df, hue_df, sat_df, value_df], axis=1)"],"metadata":{"id":"iFr-Ekg6f3Dc","executionInfo":{"status":"ok","timestamp":1689793745036,"user_tz":-120,"elapsed":7,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### Extract scene dynamics values"],"metadata":{"id":"pB1DpuqvNRX1"}},{"cell_type":"code","source":["### Define functions ###\n","########################\n","\n","# write of function to magnitude from motion vectors due to observed problem with cv2 implementation, which returns 'inf', when vectors very small\n","# -> https://github.com/opencv/opencv/issues/19506\n","# -> https://github.com/numpy/numpy/issues/5228#issue-46746558\n","def cartToPol(x, y):\n","    ang = np.arctan2(y, x)\n","    mag = np.hypot(x, y)\n","    return mag, ang\n","\n","# write own function, which attributes shot boundaries to n equal parts over the video\n","def shot_boundary_temp_distr(scene_list, n_parts):\n","\n","  # if no scenes are detected (e.g. whole video in one shot)\n","  if len(scene_list) == 0:\n","    shot_boundaries_per_part = [0,0,0]\n","    return shot_boundaries_per_part\n","\n","  # if scenes can be detected in video\n","  else:\n","    # get list of scene lens\n","    scene_len_list = np.array([(t[1]-t[0]).get_seconds() for t in scene_list])\n","\n","    # get cumsum of scene_len_list\n","    scene_len_cs = np.cumsum(scene_len_list)\n","\n","    # devide video n equal parts and get list start/stop values of video in seconds\n","    video_parts_ls = np.linspace(start = 0, stop = scene_len_cs[-1], num = n_parts + 1)\n","\n","    # save number of shot boundaries per video part\n","    shot_boundaries_per_part = []\n","    for i in range(n_parts):\n","        shot_boundaries_per_part.append(((scene_len_cs > video_parts_ls[i]) & (scene_len_cs <= video_parts_ls[i+1])).sum())\n","\n","    # return list of shot boundary counts per second for each video part\n","    return list(np.array(shot_boundaries_per_part) / (scene_len_cs[-1] / 3))\n","\n","\n","\n","# function to detect average optical flow per scene\n","# -> https://www.geeksforgeeks.org/python-opencv-dense-optical-flow/\n","def average_optical_flow(video, scene_list):\n","\n","  # initialize video capturing\n","  cap = cv2.VideoCapture(video)\n","\n","  # if no scene can be detected (e.g. whole video in one shot)\n","  if len(scene_list) == 0:\n","\n","    # extract number of frames in the video\n","    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","    # split the video in 3 equal parts\n","    equal_parts = list(np.linspace(0, num_frames, 4, dtype = int))\n","\n","    # extract tupels of first and last scene for each part\n","    scene_list = [(start, stop) for start, stop in zip(equal_parts, equal_parts[1:])]\n","\n","    # get list of start frames of each part\n","    scene_start_frames = [i[0] for i in scene_list]\n","\n","    # get list of part lengths in num frames\n","    scene_len_in_frames = [i[1] - i[0] for i in scene_list]\n","\n","\n","\n","  else:\n","\n","    # get list of start frames for each scene\n","    scene_start_frames = [i[0].get_frames() for i in scene_list]\n","\n","    # get list of scene lenths in num frames\n","    scene_len_in_frames = [i[1].get_frames() - i[0].get_frames() for i in scene_list]\n","\n","\n","\n","  ### initialize empty list and variables\n","  of_per_scene = [] # emplty list to store average of values per scene\n","  fps = int(cap.get(cv2.CAP_PROP_FPS))\n","  frame_interval = int(fps/4) # set interval of frames to consider\n","  min_scene_len =   int(fps/2) # set min scene lenth to look at (need to be as long to extract at least two frames given frame_interval)\n","\n","\n","  # loop to access start and len of individual parts\n","  for start_frame, len_in_frames in zip(scene_start_frames, scene_len_in_frames):\n","\n","    # continue if scenes are to short\n","    if len_in_frames < min_scene_len:\n","      of_per_scene.append(np.nan) # append nan to not consider in np.nanmean() function later\n","      continue\n","\n","    # set video to beginning of respective scene\n","    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n","\n","\n","    # Initialize the variables for calculating running total optical flow over respective part\n","    total_flow_x = 0\n","    total_flow_y = 0\n","    num_frames = 0\n","\n","\n","    # Loop through each frame of the video part\n","    for f in range(len_in_frames):\n","\n","        # Read frames\n","        ret, frame = cap.read()\n","\n","        # If there are no more frames, break out of the loop\n","        if not ret:\n","            break\n","\n","        # only contider frames that are start points of given frame interval\n","        if (f+1) % frame_interval != 0: # add one frame (f+1) to not exactly catch the shot boundary at beginning of scene, which would distort the optical flow calculation\n","            continue\n","\n","        # Convert the frame to grayscale\n","        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","        # If this is not the first frame, compute the optical flow\n","        if num_frames > 0:\n","\n","            # calculate flow values for each pixel\n","            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n","\n","            # Split the flow into x and y components\n","            flow_x = flow[...,0]\n","            flow_y = flow[...,1]\n","\n","            # Add the flow to the running total\n","            total_flow_x += flow_x\n","            total_flow_y += flow_y\n","\n","        # Save the current frame as the previous frame for the next iteration\n","        prev_gray = gray.copy()\n","        num_frames += 1\n","\n","    # Compute the average optical flow per part over considered frames\n","    avg_flow_x = (total_flow_x / num_frames)\n","    avg_flow_y = (total_flow_y / num_frames)\n","\n","    # compute the magnitude of the flow vector for each pixel\n","    avg_flow_mag, _ = cartToPol(avg_flow_x, avg_flow_y)\n","\n","    # append the average over all pixels\n","    of_per_scene.append(np.mean(avg_flow_mag))\n","\n","  return of_per_scene\n","\n","\n","# function to calc the optical flow over n eqal parts of the video\n","def avg_flow_temp_distr(shot_boundary_counts, of_per_scene):\n","\n","  # get cumulative distribution of scene_counts with start value of 0\n","  cdf = np.zeros(len(shot_boundary_counts) + 1).astype(int)\n","  cdf[1:] = np.cumsum(shot_boundary_counts)\n","\n","  # get mean value of portion of scenes wrt. current video part (n_part specified while for scene count function)\n","  # the flow of scenes that ovelap two parts are attributed to the latter one\n","  avg_flow_per_part = []\n","  for start, stop in zip(cdf, cdf[1:]):\n","\n","    if start == stop: # if one parts contains no shot boundary (i.e. the scene is longer than the whole part), the average optical flow of this scene is saved for the part\n","      avg_flow_per_part.append(np.nanmean(of_per_scene[start]))\n","\n","    else: # if there are multiple scenes in one part, the average over the scenes is saved (this is the most common case for the youtube short videos)\n","      avg_flow_per_part.append(np.nanmean(of_per_scene[start:stop]))\n","\n","  return avg_flow_per_part"],"metadata":{"id":"T4BLTaIF6hXq","executionInfo":{"status":"ok","timestamp":1689793745037,"user_tz":-120,"elapsed":6,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["### Apply ###\n","#############\n","\n","video_id_list = []\n","sb_list = []\n","flow_list = []\n","\n","n_parts = 3\n","\n","for video_file in tqdm(os.listdir('./Video')):\n","\n","  # save video id\n","  video_id_list.append(video_file[:-4])\n","\n","  # path to current video\n","  current_video = os.path.join('./Video', video_file)\n","\n","  # extract scene list of current video\n","  scene_list = detect(current_video, ContentDetector())\n","\n","  # extract scene counts over n parts of the video\n","  sb_counts = shot_boundary_temp_distr(scene_list, n_parts = n_parts)\n","  sb_list.append(sb_counts)\n","\n","  # extract optical flow over n parts of the video\n","  if len(scene_list) == 0:\n","    flow_list.append(average_optical_flow(current_video, scene_list))\n","\n","  else:\n","    flow_list.append(avg_flow_temp_distr(sb_counts, average_optical_flow(current_video, scene_list)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1BVANUhAXqUH","outputId":"f0575eb7-1e2c-4ccf-d9e5-247ccd7d17f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/182 [00:00<?, ?it/s]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"]}]},{"cell_type":"code","source":["# Create video id df\n","video_id_df = pd.Series(video_id_list, name = 'video_id')\n","\n","# Create shot boundary df\n","sb_df = pd.DataFrame(sb_list)\n","sb_name_dict = {i:f\"sb_part_{i+1}\" for i in range(n_parts)}\n","sb_df = sb_df.rename(columns = sb_name_dict)\n","\n","# Create  optical flow df\n","flow_df = pd.DataFrame(flow_list)\n","flow_name_dict = {i:f\"flow_part_{i+1}\" for i in range(n_parts)}\n","flow_df = flow_df.rename(columns = flow_name_dict)\n","\n","# Merge into one df\n","dynamics_df = pd.concat([video_id_df, sb_df, flow_df], axis=1)"],"metadata":{"id":"AVFb-UJ6aQI7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dynamics_df"],"metadata":{"id":"MUl87FApDyZ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Audio values"],"metadata":{"id":"Ac1WcDP4bPqM"}},{"cell_type":"code","source":["def low_level_audio_features(audio_data, n_parts):\n","\n","  # define variables\n","  FRAME_SIZE = 1024\n","  HOP_LENGTH = 512\n","\n","  # Root mean squared energy (from spectrogram -> more accurate)\n","  S, phase = librosa.magphase(librosa.stft(audio_data, n_fft=FRAME_SIZE, hop_length=HOP_LENGTH))\n","  rmse_audio = librosa.feature.rms(S=S, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)[0]\n","\n","  # Zero crossing rate\n","  zcr_audio = librosa.feature.zero_crossing_rate(audio_data, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)[0]\n","\n","  # Spectral centroid\n","  sc_audio = librosa.feature.spectral_centroid(y=audio_data, sr=sr, n_fft=FRAME_SIZE, hop_length=HOP_LENGTH)[0]\n","\n","  # Spectral bandwidth\n","  sbw_audio = librosa.feature.spectral_bandwidth(y=audio_data, sr=sr, n_fft=FRAME_SIZE, hop_length=HOP_LENGTH)[0]\n","\n","  # extract mean values per measure for each video part\n","  part_len = int(len(rmse_audio) / n_parts)\n","\n","\n","  average_rmse = []\n","  average_zcr = []\n","  average_sc = []\n","  average_sbw = []\n","\n","  for i in range(n_parts):\n","    average_rmse.append(rmse_audio[i*part_len: (i+1) * part_len].mean())\n","    average_zcr.append(zcr_audio[i*part_len: (i+1) * part_len].mean())\n","    average_sc.append(sc_audio[i*part_len: (i+1) * part_len].mean())\n","    average_sbw.append(sbw_audio[i*part_len: (i+1) * part_len].mean())\n","\n","\n","  return average_rmse, average_zcr, average_sc, average_sbw"],"metadata":{"id":"37l8UHDpFIpK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Apply function to all videos ###\n","###################################\n","\n","# Create empty lists to store\n","video_id_list = []\n","rmse_list = []\n","zcr_list = []\n","sc_list = []\n","sbw_list = []\n","\n","# Loop through audio files and extract color values\n","for audio_file in tqdm(os.listdir('./Audio')):\n","  audio_data, sr = librosa.load(os.path.join('./Audio', audio_file))\n","  average_rmse, average_zcr, average_sc, average_sbw = low_level_audio_features(audio_data, n_parts = 3)\n","\n","  video_id_list.append(audio_file[:-4])\n","  rmse_list.append(average_rmse)\n","  zcr_list.append(average_zcr)\n","  sc_list.append(average_sc)\n","  sbw_list.append(average_sbw)"],"metadata":{"id":"1apk-K4DJkNJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Create pandas dataframe ###\n","###############################\n","\n","# video id df\n","video_id_df = pd.Series(video_id_list, name = 'video_id')\n","\n","# create rmse df\n","rmse_df = pd.DataFrame(rmse_list)\n","rmse_name_dict = {i:f\"rmse_part_{i+1}\" for i in range(n_parts)}\n","rmse_df = rmse_df.rename(columns = rmse_name_dict)\n","\n","# create zcr df\n","zcr_df = pd.DataFrame(zcr_list)\n","zcr_name_dict = {i:f\"zcr_part_{i+1}\" for i in range(n_parts)}\n","zcr_df = zcr_df.rename(columns = zcr_name_dict)\n","\n","# create sc df\n","sc_df = pd.DataFrame(np.array(sc_list) / 11025)\n","sc_name_dict = {i:f\"sc_part_{i+1}\" for i in range(n_parts)}\n","sc_df = sc_df.rename(columns = sc_name_dict)\n","\n","# create sbw df\n","sbw_df = pd.DataFrame(np.array(sbw_list) / 11025)\n","sbw_name_dict = {i:f\"sbw_part_{i+1}\" for i in range(n_parts)}\n","sbw_df = sbw_df.rename(columns = sbw_name_dict)\n","\n","# merge into one df\n","low_level_audio_df = pd.concat([video_id_df, rmse_df, zcr_df, sc_df, sbw_df], axis=1)"],"metadata":{"id":"gx8aVMlwLZ9N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Merge all 3 dataframes into one ###\n","#######################################\n","\n","# merge\n","low_level_features = color_df.merge(dynamics_df, on='video_id').merge(low_level_audio_df, on='video_id')\n","\n","# move video_id column to beginning\n","first_column = low_level_features.pop('video_id')\n","low_level_features.insert(0, 'video_id', first_column)"],"metadata":{"id":"jY--kJr1VjR5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Save as csv file ###\n","########################\n","save_dir = '/content/drive/MyDrive/0_Masterarbeit/2_Pipelines/Feature_outputs'\n","\n","low_level_features.to_csv(f'./low_level_features_{data_file}.csv')\n","shutil.copy(f'./low_level_features_{data_file}.csv', save_dir)"],"metadata":{"id":"LsnGDet1WBoN"},"execution_count":null,"outputs":[]}]}