{"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"9vNBNps5fnS4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zk9_ZongDh8P"},"outputs":[],"source":["### Imports general ###\n","#######################\n","\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import shutil\n","import os\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36854,"status":"ok","timestamp":1687493272821,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"},"user_tz":-120},"id":"tWXTZG4JEUjE","outputId":"4b02028d-507a-47f8-9512-4d10ebcf185d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["### Set data directory\n","##################\n","\n","# connect to drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# set data directory\n","data_dir = '/content/drive/MyDrive/0_Masterarbeit/5_Pipelines/Data'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4TV3eP3HEggI"},"outputs":[],"source":["### Upload video and audio files ###\n","####################################\n","\n","data_file = 'salomon'\n","\n","# copy zip files\n","shutil.copy(os.path.join(data_dir, f'Video_{data_file}.zip'), './')\n","shutil.copy(os.path.join(data_dir, f'Audio_{data_file}.zip'), './')\n","\n","# create folders to unpack zip files to\n","os.makedirs('./Video')\n","os.makedirs('./Audio')\n","\n","# unpack zip files\n","shutil.unpack_archive(f'./Video_{data_file}.zip', extract_dir = './Video')\n","shutil.unpack_archive(f'./Audio_{data_file}.zip', extract_dir = './Audio')"]},{"cell_type":"markdown","metadata":{"id":"VKVqeT5uG4R5"},"source":["### Action detection"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6455,"status":"ok","timestamp":1687493296428,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"},"user_tz":-120},"id":"1cIGI1ujHZOJ","outputId":"d6b6acd8-1a00-4b8f-ea12-e8bfdd98aa49"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scenedetect[opencv]\n","  Downloading scenedetect-0.6.1-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Click in /usr/local/lib/python3.10/dist-packages (from scenedetect[opencv]) (8.1.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scenedetect[opencv]) (1.22.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scenedetect[opencv]) (4.65.0)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from scenedetect[opencv]) (1.4.4)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from scenedetect[opencv]) (4.7.0.72)\n","Installing collected packages: scenedetect\n","Successfully installed scenedetect-0.6.1\n"]}],"source":["### Installation scene detection ###\n","####################################\n","\n","!pip install --upgrade scenedetect[opencv]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1188,"status":"ok","timestamp":1687493297612,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"},"user_tz":-120},"id":"wX_jlulLHD5P","outputId":"7c2a2cbd-40e6-4a9e-8e0d-bea517a2b64a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'moments_models'...\n","remote: Enumerating objects: 139, done.\u001b[K\n","remote: Counting objects: 100% (60/60), done.\u001b[K\n","remote: Compressing objects: 100% (17/17), done.\u001b[K\n","remote: Total 139 (delta 53), reused 43 (delta 43), pack-reused 79\u001b[K\n","Receiving objects: 100% (139/139), 58.78 KiB | 19.59 MiB/s, done.\n","Resolving deltas: 100% (75/75), done.\n"]}],"source":["### Clone git repo ###\n","######################\n","\n","# clone\n","!git clone https://github.com/metalbubble/moments_models.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gg2nGUySHOvI"},"outputs":[],"source":["### Imports for action detection ###\n","####################################\n","\n","# scene detection\n","from scenedetect import detect, ContentDetector\n","\n","# action detection model\n","from moments_models import models\n","import torch\n","from torch.nn import functional as F\n","from torchvision import transforms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNenji_ZIAeF"},"outputs":[],"source":["### Load model ###\n","##################\n","\n","# model\n","model = models.load_model('multi_resnet3d50')\n","\n","# categories\n","categories = models.load_categories('./moments_models/category_multi_momentsv2.txt')\n","\n","# load transform\n","transform = transforms.Compose([transforms.ToPILImage(),\n","                                transforms.Resize((224, 224)),\n","                                transforms.ToTensor(),\n","                                transforms.Normalize([0.485, 0.456, 0.406],\n","                                                     [0.229, 0.224, 0.225])])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G_0vofewIfTN"},"outputs":[],"source":["### Function to apply action detection model to individual scenes ###\n","#####################################################################\n","\n","def action_detection(video_file):\n","\n","\n","  ### Scelect sample frames for each video ###\n","  ############################################\n","\n","  # detect scenes\n","  scene_list = detect(video_file, ContentDetector())\n","\n","  # if no scenes detected -> eg. whole video in one shot\n","  if len(scene_list) == 0:\n","\n","    # extract number of frames in the video\n","    cap = cv2.VideoCapture(video_file)\n","    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    cap.release()\n","\n","\n","\n","    if num_frames >= 144:\n","\n","      # split the video in 9 equal parts\n","      equal_parts = list(np.linspace(0, num_frames, 10, dtype = int))\n","\n","      # extract tupels of first and last scene for each part\n","      scene_list = [(start, stop) for start, stop in zip(equal_parts, equal_parts[1:])]\n","\n","      # select every third part to be analyzed\n","      scene_list = [scene_list[j] for j in range(0, len(scene_list), 3)]\n","\n","    else:\n","      # split the video in 6 equal parts\n","      equal_parts = list(np.linspace(0, num_frames, 7, dtype = int))\n","\n","      # extract tupels of first and last scene for each part\n","      scene_list = [(start, stop) for start, stop in zip(equal_parts, equal_parts[1:])]\n","\n","      # select every third part to be analyzed\n","      scene_list = [scene_list[j] for j in range(0, len(scene_list), 2)]\n","\n","\n","    # get list of start frames of each part\n","    scene_start_frames = [i[0] for i in scene_list]\n","\n","    # get list of part lengths in num frames\n","    scene_len_in_frames = [i[1] - i[0] for i in scene_list]\n","\n","  else: # scenes detected\n","\n","    # select every third scene to be analyzed\n","    scene_list = [scene_list[j] for j in range(0, len(scene_list), 3)]\n","\n","    # get list of start frames for each scene\n","    scene_start_frames = [i[0].get_frames() for i in scene_list]\n","\n","    # get list of scene lengths in num frames\n","    scene_len_in_frames = [i[1].get_frames() - i[0].get_frames() for i in scene_list]\n","\n","\n","  # get list of 16 subsequent sample frames for each scene as required by the model\n","  sample_frames_per_scene = []\n","  for i, j in zip(scene_start_frames, scene_len_in_frames):\n","\n","    if j >= 16: # only consider scenes that include at least 16 frames\n","      sample_frames_per_scene.append(\n","         list(np.linspace(i, i+j-1, 16, dtype = int))\n","         )\n","\n","  ### Extract the selected sample frames per scene ###\n","  ####################################################\n","\n","  cap = cv2.VideoCapture(video_file)\n","\n","  scene_frames = []\n","  for l in sample_frames_per_scene:\n","\n","    rgb_frames = []\n","    for f in l:\n","      # set to position of respective sample frame\n","      cap.set(cv2.CAP_PROP_POS_FRAMES, f)\n","\n","      # Read the frame from the video\n","      ret, frame = cap.read()\n","\n","      # convert frame array to RGB format\n","      img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","      rgb_frames.append(img)\n","\n","    scene_frames.append(rgb_frames)\n","\n","\n","  # Release the video capture object and close the windows\n","  cap.release()\n","  cv2.destroyAllWindows()\n","\n","\n","\n","  ### Predict category probabilities and average over video ###\n","  #############################################################\n","\n","  for i, frs in enumerate(scene_frames):\n","\n","    # create transformed model input of 16 subsequent frames per scene to get prediction for scene\n","    input = torch.stack([transform(frame) for frame in frs], 1).unsqueeze(0)\n","\n","    # Make video prediction\n","    with torch.no_grad():\n","      logits = model(input) # extract logits\n","      h_x = F.softmax(logits, 1).mean(dim=0) # convert logits to class probabilities\n","\n","    # sum class probabilities over all scenes\n","    if i==0:\n","      average_probs = h_x\n","\n","    else:\n","      average_probs += h_x\n","\n","  # average class probabilities by number of considered scenes\n","  average_probs /= len(sample_frames_per_scene)\n","\n","  return average_probs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rSCSt4SSccXo","executionInfo":{"status":"ok","timestamp":1687494176953,"user_tz":-120,"elapsed":808954,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"outputId":"053501c5-ee8a-4fc5-f7b8-69c4e4489646"},"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/48 [00:00<?, ?it/s]INFO:pyscenedetect:Detecting scenes...\n","  2%|▏         | 1/48 [00:12<09:58, 12.74s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n","  4%|▍         | 2/48 [00:39<16:01, 20.90s/it]INFO:pyscenedetect:Detecting scenes...\n","  6%|▋         | 3/48 [00:49<11:57, 15.94s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n","  8%|▊         | 4/48 [01:18<15:38, 21.34s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 10%|█         | 5/48 [01:34<13:44, 19.18s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 12%|█▎        | 6/48 [02:06<16:32, 23.63s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 15%|█▍        | 7/48 [02:22<14:19, 20.97s/it]INFO:pyscenedetect:Detecting scenes...\n"," 17%|█▋        | 8/48 [02:31<11:37, 17.44s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 19%|█▉        | 9/48 [02:34<08:23, 12.92s/it]INFO:pyscenedetect:Detecting scenes...\n"," 21%|██        | 10/48 [02:45<07:39, 12.08s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 23%|██▎       | 11/48 [02:59<07:52, 12.78s/it]INFO:pyscenedetect:Detecting scenes...\n"," 25%|██▌       | 12/48 [03:09<07:13, 12.03s/it]INFO:pyscenedetect:Downscale factor set to 5, effective resolution: 256 x 144\n","INFO:pyscenedetect:Detecting scenes...\n"," 27%|██▋       | 13/48 [03:23<07:13, 12.37s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 29%|██▉       | 14/48 [03:52<09:54, 17.49s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 31%|███▏      | 15/48 [04:20<11:27, 20.82s/it]INFO:pyscenedetect:Detecting scenes...\n"," 33%|███▎      | 16/48 [04:26<08:43, 16.37s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 35%|███▌      | 17/48 [04:44<08:34, 16.60s/it]INFO:pyscenedetect:Detecting scenes...\n"," 38%|███▊      | 18/48 [04:54<07:21, 14.72s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 40%|███▉      | 19/48 [05:32<10:27, 21.63s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 42%|████▏     | 20/48 [05:50<09:34, 20.51s/it]INFO:pyscenedetect:Detecting scenes...\n"," 44%|████▍     | 21/48 [05:55<07:14, 16.11s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 46%|████▌     | 22/48 [06:17<07:38, 17.62s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 303 x 539\n","INFO:pyscenedetect:Detecting scenes...\n"," 48%|████▊     | 23/48 [06:23<06:00, 14.42s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 50%|█████     | 24/48 [06:39<05:50, 14.62s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 52%|█████▏    | 25/48 [06:49<05:04, 13.26s/it]INFO:pyscenedetect:Detecting scenes...\n"," 54%|█████▍    | 26/48 [06:57<04:19, 11.79s/it]INFO:pyscenedetect:Detecting scenes...\n"," 56%|█████▋    | 27/48 [07:05<03:46, 10.77s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 355 x 360\n","INFO:pyscenedetect:Detecting scenes...\n"," 58%|█████▊    | 28/48 [07:52<07:09, 21.47s/it]INFO:pyscenedetect:Downscale factor set to 5, effective resolution: 256 x 144\n","INFO:pyscenedetect:Detecting scenes...\n"," 60%|██████    | 29/48 [08:26<08:00, 25.29s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 62%|██████▎   | 30/48 [08:46<07:05, 23.64s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 304 x 540\n","INFO:pyscenedetect:Detecting scenes...\n"," 65%|██████▍   | 31/48 [09:00<05:51, 20.67s/it]INFO:pyscenedetect:Detecting scenes...\n"," 67%|██████▋   | 32/48 [09:12<04:52, 18.28s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 69%|██████▉   | 33/48 [09:37<05:01, 20.12s/it]INFO:pyscenedetect:Detecting scenes...\n"," 71%|███████   | 34/48 [09:41<03:34, 15.35s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 73%|███████▎  | 35/48 [09:48<02:47, 12.89s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 75%|███████▌  | 36/48 [09:56<02:15, 11.29s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 77%|███████▋  | 37/48 [10:39<03:50, 20.94s/it]INFO:pyscenedetect:Detecting scenes...\n"," 79%|███████▉  | 38/48 [10:43<02:37, 15.78s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 81%|████████▏ | 39/48 [10:57<02:17, 15.23s/it]INFO:pyscenedetect:Detecting scenes...\n"," 83%|████████▎ | 40/48 [11:07<01:49, 13.73s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 85%|████████▌ | 41/48 [11:35<02:06, 18.11s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 88%|████████▊ | 42/48 [11:44<01:32, 15.38s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 90%|████████▉ | 43/48 [12:01<01:19, 15.84s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 92%|█████████▏| 44/48 [12:08<00:52, 13.22s/it]INFO:pyscenedetect:Detecting scenes...\n"," 94%|█████████▍| 45/48 [12:20<00:38, 12.70s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 96%|█████████▌| 46/48 [12:43<00:31, 15.84s/it]INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 360 x 640\n","INFO:pyscenedetect:Detecting scenes...\n"," 98%|█████████▊| 47/48 [13:23<00:23, 23.05s/it]INFO:pyscenedetect:Detecting scenes...\n","100%|██████████| 48/48 [13:28<00:00, 16.85s/it]\n"]}],"source":["### Extract for each video ###\n","##############################\n","\n","video_id = []\n","action_probs = []\n","for video_file in tqdm(os.listdir('./Video')):\n","\n","  video_id.append(video_file[:-4])\n","  action_probs.append(action_detection(os.path.join('./Video', video_file)).tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jt3OLvDifT-m"},"outputs":[],"source":["### Create final dataframe for action detection ###\n","###################################################\n","\n","action_df = pd.DataFrame(action_probs)\n","action_name_dict = {i:f\"p_action_{c}\" for i,c in enumerate(categories)}\n","action_df = action_df.rename(columns = action_name_dict)\n","action_df['video_id'] = video_id"]},{"cell_type":"markdown","metadata":{"id":"xvBNUpevhQGY"},"source":["### Face expression detection ###"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CJhBRwtbhPiA","executionInfo":{"status":"ok","timestamp":1687494191892,"user_tz":-120,"elapsed":14945,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"outputId":"477251d3-ec30-4198-b5fc-8ea404d8bdb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting face_detection\n","  Downloading face_detection-0.2.2.tar.gz (20 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from face_detection) (2.0.1+cu118)\n","Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from face_detection) (0.15.2+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_detection) (1.22.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->face_detection) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->face_detection) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->face_detection) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->face_detection) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->face_detection) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->face_detection) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->face_detection) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->face_detection) (16.0.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.3.0->face_detection) (2.27.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.3.0->face_detection) (8.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->face_detection) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.3.0->face_detection) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.3.0->face_detection) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.3.0->face_detection) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.3.0->face_detection) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->face_detection) (1.3.0)\n","Building wheels for collected packages: face_detection\n","  Building wheel for face_detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for face_detection: filename=face_detection-0.2.2-py3-none-any.whl size=25560 sha256=2103a9396fdfc6bf5f13576c03f46fb0d498ec3dd6d3f7ef2cf4bfcab409482f\n","  Stored in directory: /root/.cache/pip/wheels/f9/14/a1/617e184738e71e46c1e75f068f67a911917ae5d02faeabc4e4\n","Successfully built face_detection\n","Installing collected packages: face_detection\n","Successfully installed face_detection-0.2.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting deepface\n","  Downloading deepface-0.0.79-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (1.22.4)\n","Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from deepface) (1.5.3)\n","Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (4.65.0)\n","Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from deepface) (4.6.6)\n","Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (8.4.0)\n","Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.10/dist-packages (from deepface) (4.7.0.72)\n","Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.12.0)\n","Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.12.0)\n","Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.2.4)\n","Collecting mtcnn>=0.1.0 (from deepface)\n","  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting retina-face>=0.0.1 (from deepface)\n","  Downloading retina_face-0.0.13-py3-none-any.whl (16 kB)\n","Collecting fire>=0.4.0 (from deepface)\n","  Downloading fire-0.5.0.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gunicorn>=20.1.0 (from deepface)\n","  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire>=0.4.0->deepface) (1.16.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.4.0->deepface) (2.3.0)\n","Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (2.3.0)\n","Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (3.1.2)\n","Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (2.1.2)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (8.1.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->deepface) (3.12.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->deepface) (2.27.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->deepface) (4.11.2)\n","Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.10/dist-packages (from gunicorn>=20.1.0->deepface) (67.7.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2022.7.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (23.3.3)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.54.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (3.8.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.4.10)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (16.0.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (23.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (3.20.3)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (2.12.2)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (2.12.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.32.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.40.0)\n","Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=1.9.0->deepface) (0.1.0)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=1.9.0->deepface) (1.10.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=1.1.2->deepface) (2.1.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (3.4.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (0.7.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (1.8.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.4.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (3.4)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (1.3.1)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (3.2.2)\n","Building wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=4c8091a8d8f387699420f4f0348bcbe8efedcc034fe3f012e0e197354810a8fe\n","  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n","Successfully built fire\n","Installing collected packages: gunicorn, fire, mtcnn, retina-face, deepface\n","Successfully installed deepface-0.0.79 fire-0.5.0 gunicorn-20.1.0 mtcnn-0.1.1 retina-face-0.0.13\n"]}],"source":["### Installations face expression detection ###\n","###############################################\n","\n","!pip install face_detection\n","!pip install deepface"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m4LaBTOuhscN","executionInfo":{"status":"ok","timestamp":1687494194712,"user_tz":-120,"elapsed":2827,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"outputId":"6b9f6267-8109-4a2c-a0ed-21632c2d3b10"},"outputs":[{"output_type":"stream","name":"stdout","text":["Directory  /root /.deepface created\n","Directory  /root /.deepface/weights created\n"]}],"source":["### Imports face expression detection ###\n","#########################################\n","\n","import face_detection\n","from deepface import DeepFace"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ChWKOJMXh1-Y","executionInfo":{"status":"ok","timestamp":1687494215021,"user_tz":-120,"elapsed":20314,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"outputId":"4216c5f2-67e7-4491-98cf-610e97ceb538"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://folk.ntnu.no/haakohu/RetinaFace_ResNet50.pth\" to /root/.cache/torch/hub/checkpoints/RetinaFace_ResNet50.pth\n","100%|██████████| 104M/104M [00:10<00:00, 10.8MB/s]\n"]}],"source":["### Load model ###\n","##################\n","\n","face_detector = face_detection.build_detector('RetinaNetResNet50', confidence_threshold=.8, nms_iou_threshold=.3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOlDEOPhiFWd"},"outputs":[],"source":["### Function to detect faces in video and apply expression recognition  ###\n","###########################################################################\n","\n","def face_exp_detection(video_file):\n","\n","    # initialize video capturing object\n","    cap = cv2.VideoCapture(video_file)\n","\n","    # extract fps to set interval between frames to be contidered\n","    fps = int(cap.get(cv2.CAP_PROP_FPS))\n","\n","    # frame interval -> every n = 2 second, a frame is considered in prediction\n","    frame_interval = 2 * fps\n","\n","    # initialize counter and emotion list object\n","    counter = 0\n","    expression_list = []\n","\n","    # loop though video\n","    while True:\n","      ret, frame = cap.read()\n","\n","      counter+=1\n","\n","      if not ret:\n","        break\n","\n","      if counter % frame_interval != 0:\n","        continue\n","\n","      # detect faces in frame\n","      det = face_detector.detect(frame)\n","\n","      # if no face detected continue with next frame\n","      if len(det) == 0:\n","        continue\n","\n","      # crop faces from frames and apply emotion classification\n","      for bbox in det:\n","\n","        # crop for face\n","        xmin, ymin, xmax, ymax , _ = bbox\n","        face = frame[abs(int(ymin)):abs(int(ymax)), abs(int(xmin)):abs(int(xmax))]\n","\n","        # apply emotion detection\n","        expression_det = DeepFace.analyze(face, actions = 'emotion', enforce_detection= False, silent = True)\n","\n","        expression_dict = expression_det[0]['emotion']\n","\n","        expression_list.append([expression_dict[emo] for emo in expression_dict])\n","\n","    # Release the video capture object and close the windows\n","    cap.release()\n","    cv2.destroyAllWindows()\n","\n","    # if no face detected in the entire video -> return expression list of all 0's\n","    if len(expression_list) == 0:\n","       return [0,0,0,0,0,0,0]\n","\n","    # if one faces detected in video -> take mean over all class probabilities and devide by 100 (since model scales probs by 100)\n","    else:\n","      return list(np.array(expression_list).mean(0) / 100)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xww9vfO1i6c5","executionInfo":{"status":"ok","timestamp":1687494381989,"user_tz":-120,"elapsed":166993,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"outputId":"74d691a0-e219-412f-c1d2-da484a8863e4"},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/48 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["facial_expression_model_weights.h5 will be downloaded...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://github.com/serengil/deepface_models/releases/download/v1.0/facial_expression_model_weights.h5\n","To: /root/.deepface/weights/facial_expression_model_weights.h5\n","\n","  0%|          | 0.00/5.98M [00:00<?, ?B/s]\u001b[A\n"," 18%|█▊        | 1.05M/5.98M [00:00<00:00, 10.2MB/s]\u001b[A\n"," 35%|███▌      | 2.10M/5.98M [00:00<00:00, 6.56MB/s]\u001b[A\n","100%|██████████| 5.98M/5.98M [00:00<00:00, 10.6MB/s]\n","100%|██████████| 48/48 [02:46<00:00,  3.48s/it]\n"]}],"source":["### Extract for each video ###\n","##############################\n","\n","video_id = []\n","exp_video_list = []\n","\n","for video_file in tqdm(os.listdir('./Video')):\n","\n","  video_id.append(video_file[:-4])\n","  exp_video_list.append(face_exp_detection(os.path.join('./Video', video_file)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jXeotozSjgvt"},"outputs":[],"source":["### Create final dataframe for face expression detection ###\n","############################################################\n","# create df\n","face_exp_df = pd.DataFrame(exp_video_list)\n","\n","# create dict of col names\n","exp_classes = [\"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"]\n","exp_name_dict = {i:f\"p_face_{c}\" for i,c in enumerate(exp_classes)}\n","\n","# add names and video_id column\n","face_exp_df = face_exp_df.rename(columns = exp_name_dict)\n","face_exp_df['video_id'] = video_id"]},{"cell_type":"markdown","metadata":{"id":"um4YMm5_nEal"},"source":["### Scene Detection ###"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UOgnGadJnC0q","executionInfo":{"status":"ok","timestamp":1687494395354,"user_tz":-120,"elapsed":13374,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"outputId":"7a083349-e1b0-45b7-84c0-e6ee25869267"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==4.28.0\n","  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.28.0)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.0)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 tokenizers-0.13.3 transformers-4.28.0\n"]}],"source":["### Installations scene detection ###\n","#####################################\n","\n","!pip uninstall -y transformers\n","!pip install transformers==4.28.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gu6JzLbSnxCa"},"outputs":[],"source":["### Imports scene detection ###\n","###############################\n","\n","from transformers import ViTForImageClassification, ViTFeatureExtractor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGbYLlbQn8-M"},"outputs":[],"source":["### Load Model ###\n","##################\n","\n","# path to pre-trained model\n","scene_model_path = '/content/drive/MyDrive/0_Masterarbeit/5_Pipelines/Models/best_scene_detection_model'\n","\n","# classes to be detected\n","scene_classes = ['airport', 'alley', 'athlectic_field', 'auditorium', 'bar',\n","          'basketball_court', 'bathroom', 'beach', 'bedroom', 'bistro',\n","          'canyon', 'computer_room', 'desert', 'discotheque', 'factory',\n","          'field', 'forest', 'gym', 'harbor', 'highway', 'hill',\n","          'kitchen', 'lake', 'library', 'living_room', 'locker_room',\n","          'market', 'mountain', 'ocean', 'office', 'park', 'raceway',\n","          'river', 'skatepark', 'snowfield', 'stadium', 'street',\n","          'swimming_pool', 'tennis_court']\n","\n","# Load feature extractor\n","scene_feature_extractor = ViTFeatureExtractor.from_pretrained(scene_model_path)\n","\n","# Load model\n","scene_model = ViTForImageClassification.from_pretrained(\n","    scene_model_path,\n","    num_labels=len(scene_classes),\n","    id2label={str(i): c for i, c in enumerate(scene_classes)},\n","    label2id={c: str(i) for i, c in enumerate(scene_classes)}\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o7o-48Hso_Sj"},"outputs":[],"source":["### Function to apply scene detection to video ###\n","##################################################\n","\n","def scene_detection(video_file):\n","\n","   # initialize video capturing object\n","    cap = cv2.VideoCapture(video_file)\n","\n","    # extract fps to set interval between frames to be contidered\n","    fps = int(cap.get(cv2.CAP_PROP_FPS))\n","\n","    # frame interval -> every n = 2 second, a frame is considered in prediction\n","    frame_interval = 2 * fps\n","\n","    # initialize counter and scene list object\n","    counter = 0\n","    scene_list = []\n","\n","    # loop though video frame by frame\n","    while True:\n","      ret, frame = cap.read()\n","\n","      counter+=1\n","\n","      if not ret:\n","        break\n","\n","      # only consider first frame of every specified interval\n","      if counter % frame_interval != 0:\n","        continue\n","\n","\n","      # feature extraction\n","      inp = scene_feature_extractor(frame[:,:,::-1], return_tensors='pt')\n","\n","      # prediction\n","      with torch.no_grad():\n","\n","        # get model prediction as logits\n","        logits = scene_model(inp['pixel_values'])['logits']\n","\n","      # convert to class probabilities and save\n","      scene_list.append(logits.softmax(dim = -1)[0].tolist())\n","\n","\n","\n","    # Release the video capture object and close the windows\n","    cap.release()\n","    cv2.destroyAllWindows()\n","\n","    # return mean of class probabilites over all considered frames\n","    return list(np.array(scene_list).mean(0))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uFBXf0LEqPhH","executionInfo":{"status":"ok","timestamp":1687494875994,"user_tz":-120,"elapsed":464895,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"outputId":"e029a216-43ad-424c-848c-4ee7a5197375"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 48/48 [07:44<00:00,  9.68s/it]\n"]}],"source":["### Apply scene detection to all videos ###\n","###########################################\n","\n","video_id = []\n","scene_video_list = []\n","\n","for video_file in tqdm(os.listdir('./Video')):\n","\n","  video_id.append(video_file[:-4])\n","  scene_video_list.append(scene_detection(os.path.join('./Video', video_file)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m8feQY_crcXE"},"outputs":[],"source":["### Create final dataframe for scene detection ###\n","##################################################\n","\n","# create df\n","scene_df = pd.DataFrame(scene_video_list)\n","\n","# create dict of col names\n","scene_name_dict = {i:f\"p_scene_{c}\" for i,c in enumerate(scene_classes)}\n","\n","# add names and video_id column\n","scene_df = scene_df.rename(columns = scene_name_dict)\n","scene_df['video_id'] = video_id"]},{"cell_type":"markdown","metadata":{"id":"ZegbYD5LvsIs"},"source":["### Sound detection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2TOSUrZWv3hI"},"outputs":[],"source":["### Import sound detection ###\n","##############################\n","\n","import librosa\n","from transformers import AutoFeatureExtractor, AutoModelForAudioClassification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"noEMy_AswPgE"},"outputs":[],"source":["### Load model ###\n","##################\n","\n","# path to pre-trained model\n","sound_model_path = '/content/drive/MyDrive/0_Masterarbeit/5_Pipelines/Models/best_sound_detection_model'\n","\n","# classes to be detected\n","sound_classes = ['airplane', 'angry voice', 'breathing', 'brushing_teeth', 'calm voice',\n","                'can_opening', 'car_horn', 'cat', 'chainsaw', 'chirping_birds',\n","                'church_bells', 'clapping', 'clock_alarm', 'clock_tick', 'coughing',\n","                'cow', 'crackling_fire', 'crickets', 'crow', 'crying_baby', 'dog',\n","                'door_wood_creaks', 'door_wood_knock', 'drinking_sipping', 'engine',\n","                'fireworks', 'footsteps', 'frog', 'glass_breaking', 'hand_saw',\n","                'happy voice', 'helicopter', 'hen', 'insects', 'keyboard_typing',\n","                'laughing', 'mouse_click', 'pig', 'pouring_water', 'rain', 'rooster',\n","                'sad voice', 'sea_waves', 'sheep', 'siren', 'sneezing', 'snoring',\n","                'thunderstorm', 'toilet_flush', 'train', 'vacuum_cleaner', 'washing_machine',\n","                'water_drops', 'wind']\n","\n","# Load feature extractor\n","sound_feature_extractor = AutoFeatureExtractor.from_pretrained(sound_model_path)\n","\n","# Load model\n","sound_model = AutoModelForAudioClassification.from_pretrained(sound_model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JoaTsPA_wb4w"},"outputs":[],"source":["### Function to apply sound detection to video ###\n","##################################################\n","\n","def sound_detection(audio_file):\n","\n","  # load audio data\n","  audio_data, sr = librosa.load(audio_file)\n","\n","  # resample to sampling rate the model was trained on\n","  a_rs = librosa.resample(audio_data, orig_sr = sr, target_sr = 16000)\n","\n","  # split audio file into 10 parts of equal length\n","  parts = np.linspace(0, len(a_rs), 10)\n","\n","  # loop to consider each part individually\n","  audio_list = []\n","\n","  for i in [0, 3, 5, 7]:\n","\n","    # apply Short-time Fourier Transform to respective part to get model input\n","    inp = sound_feature_extractor(a_rs[int(parts[i]): int(parts[i+1])], sampling_rate=16000, return_tensors=\"pt\")\n","\n","    # extract class probabilities form output logits\n","    with torch.no_grad():\n","      class_probs = sound_model(**inp).logits.softmax(dim = -1)[0]\n","\n","    audio_list.append(class_probs.tolist())\n","\n","  # return mean class probabilites over all audio parts\n","  return list(np.array(audio_list).mean(0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jn_7rqN-yTvD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687495721719,"user_tz":-120,"elapsed":831864,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"outputId":"dd9e5b03-5134-4eba-b172-840797a5638f"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 48/48 [13:52<00:00, 17.34s/it]\n"]}],"source":["### Apply sound detection to all audio files ###\n","################################################\n","\n","video_id = []\n","sound_video_list = []\n","\n","for audio_file in tqdm(os.listdir('./Audio')):\n","\n","  video_id.append(audio_file[:-4])\n","  sound_video_list.append(sound_detection(os.path.join('./Audio', audio_file)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0f_nJqR4QG8"},"outputs":[],"source":["### Create final dataframe for sound detection ###\n","##################################################\n","\n","# create df\n","sound_df = pd.DataFrame(sound_video_list)\n","\n","# create dict of col names\n","sound_name_dict = {i:f\"p_sound_{c}\" for i,c in enumerate(sound_classes)}\n","\n","# add names and video_id column\n","sound_df = sound_df.rename(columns = sound_name_dict)\n","sound_df['video_id'] = video_id"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"goSSQ6qe7c9g"},"outputs":[],"source":["### Merge all 4 dataframes into one ###\n","#######################################\n","\n","# merge\n","mid_level_features = action_df.merge(face_exp_df, on='video_id').merge(scene_df, on='video_id').merge(sound_df, on = 'video_id')\n","\n","# move video_id column to beginning\n","first_column = mid_level_features.pop('video_id')\n","mid_level_features.insert(0, 'video_id', first_column)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cT178wkhuSBq","executionInfo":{"status":"ok","timestamp":1687495721722,"user_tz":-120,"elapsed":14,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"6a67a240-05b0-47a7-f5a2-e4bc096edd2b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/0_Masterarbeit/5_Pipelines/Feature_outputs/mid_level_features_salomon.csv'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":29}],"source":["### Save as csv file ###\n","########################\n","save_dir = '/content/drive/MyDrive/0_Masterarbeit/5_Pipelines/Feature_outputs'\n","\n","mid_level_features.to_csv(f'./mid_level_features_{data_file}.csv')\n","shutil.copy(f'./mid_level_features_{data_file}.csv', save_dir)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyMVgxNIHgGQ8wEA8PnquRIe"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}