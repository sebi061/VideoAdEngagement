{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyOBkruLLA2pndp71c40TbhY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"aIHIw4PkOcxe"},"outputs":[],"source":["### Imports general ###\n","#######################\n","\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import torch\n","import shutil\n","import os\n","from tqdm import tqdm"]},{"cell_type":"code","source":["### Set data directory\n","##################\n","\n","# connect to drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# set data directory\n","data_dir = '/content/drive/MyDrive/0_Masterarbeit/2_Pipelines/Data'\n","\n","# set data directory\n","model_dir = '/content/drive/MyDrive/0_Masterarbeit/2_Pipelines/Models'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z2i4fBJ_OzhL","executionInfo":{"status":"ok","timestamp":1688147621812,"user_tz":-120,"elapsed":16812,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"outputId":"a1558551-fdb2-48a0-b3bc-5fbf93605483"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["### Upload video and audio files ###\n","####################################\n","\n","data_file = 'nike_part2'\n","\n","# copy zip files\n","shutil.copy(os.path.join(data_dir, f'Video_{data_file}.zip'), './')\n","shutil.copy(os.path.join(data_dir, f'Audio_{data_file}.zip'), './')\n","\n","# create folders to unpack zip files to\n","os.makedirs('./Video')\n","os.makedirs('./Audio')\n","\n","# unpack zip files\n","shutil.unpack_archive(f'./Video_{data_file}.zip', extract_dir = './Video')\n","shutil.unpack_archive(f'./Audio_{data_file}.zip', extract_dir = './Audio')"],"metadata":{"id":"WwboJijrO3cI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Intent detection"],"metadata":{"id":"qdkkfghsPBsV"}},{"cell_type":"code","source":["### Installations intetn detection ###\n","######################################\n","\n","!pip install git+https://github.com/openai/CLIP.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fJL0_pJrPaVe","executionInfo":{"status":"ok","timestamp":1688147638062,"user_tz":-120,"elapsed":7132,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"outputId":"de7d85c2-1e2e-42e4-dfce-1c58548af415"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-cjhuengq\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-cjhuengq\n","  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ftfy (from clip==1.0)\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.2+cu118)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.6.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.27.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (8.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369370 sha256=db96e3e95dff642f995ccd4e58469a482599280ec6f4a6874da0ae996a317a61\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-k1pntj3n/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: ftfy, clip\n","Successfully installed clip-1.0 ftfy-6.1.1\n"]}]},{"cell_type":"code","source":["### Imports for intetn detection ###\n","####################################\n","\n","from PIL import Image\n","import torch\n","import clip"],"metadata":{"id":"Fu2eyLKUPNB8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Load Model ###\n","##################\n","\n","# set device\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","\n","# load model and prepocessor\n","intent_model, intent_preprocess = clip.load(\"ViT-B/32\",device=device)\n","\n","# load pre-trained model\n","checkpoint = torch.load( os.path.join(model_dir, 'best_intentonomy_model.pt'))\n","intent_model.load_state_dict(checkpoint['model_state_dict'])\n","\n","# classes\n","labels_intent = ['virtue','self-fulfill', 'openness to experience', 'security and belonging',\n","          'power', 'health', 'familiy', 'ambition and ability', 'financial and occupational success']\n","\n","# tokenize class labels\n","labels_intent_tok = clip.tokenize([f\"The picture represents {l}\" for l in labels_intent]).to(device)"],"metadata":{"id":"PW1jeXAGPZdQ","executionInfo":{"status":"ok","timestamp":1688147674376,"user_tz":-120,"elapsed":35963,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0ddccf28-5db3-4444-b99d-91b875ba9f5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 338M/338M [00:06<00:00, 51.1MiB/s]\n"]}]},{"cell_type":"code","source":["### Function to apply intent detection to individal frames of the video ###\n","###########################################################################\n","\n","def intent_detection(video_file):\n","\n","   # initialize video capturing object\n","    cap = cv2.VideoCapture(video_file)\n","\n","    # extract fps to set interval between frames to be contidered\n","    fps = int(cap.get(cv2.CAP_PROP_FPS))\n","\n","    # frame interval -> every n = 2 second, a frame is considered in prediction\n","    frame_interval = 2 * fps\n","\n","    # initialize counter and emotion list object\n","    counter = 0\n","    intent_list = []\n","\n","    # loop though video\n","    while True:\n","      ret, frame = cap.read()\n","\n","      counter+=1\n","\n","      if not ret:\n","        break\n","\n","      if counter % frame_interval != 0:\n","        continue\n","\n","\n","      # feature extraction\n","      img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","      img_pre_process = intent_preprocess(Image.fromarray(img)).unsqueeze(0).to(device)\n","\n","      # prediction\n","      with torch.no_grad():\n","            logits , _ = intent_model(img_pre_process, labels_intent_tok)\n","            p = logits.softmax(dim=-1)\n","\n","      intent_list.append(p[0].tolist())\n","\n","\n","\n","    # Release the video capture object and close the windows\n","    cap.release()\n","    cv2.destroyAllWindows()\n","\n","    return list(np.array(intent_list).mean(0))"],"metadata":{"id":"VFgZHiyxRO23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Apply to all videos ###\n","###########################\n","\n","video_id = []\n","intent_video_list = []\n","\n","for video_file in tqdm(os.listdir('./Video')):\n","\n","  video_id.append(video_file[:-4])\n","  intent_video_list.append(intent_detection(os.path.join('./Video', video_file)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DddReYcLRkgY","executionInfo":{"status":"ok","timestamp":1688147755614,"user_tz":-120,"elapsed":81259,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"outputId":"a919d3cd-42a0-4ff8-ada3-95ed0d76c97d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [01:21<00:00,  2.03s/it]\n"]}]},{"cell_type":"code","source":["### Create final dataframe for intent detection ###\n","###################################################\n","\n","intent_df = pd.DataFrame(intent_video_list, columns=  [f\"p_intent_{l}\" for l in labels_intent])\n","intent_df['video_id'] = video_id"],"metadata":{"id":"fsx8x4vhRrgs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Memorability classification"],"metadata":{"id":"QksIkYkvTTx_"}},{"cell_type":"code","source":["### Installations memorability classification ###\n","#################################################\n","\n","!pip uninstall -y transformers\n","!pip install transformers==4.28.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HoxlShlaTdPW","executionInfo":{"status":"ok","timestamp":1688147766304,"user_tz":-120,"elapsed":10707,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"outputId":"08fd91ad-a6fe-4674-b463-378d17f09333"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mCollecting transformers==4.28.0\n","  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.28.0)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.0)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.6.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 tokenizers-0.13.3 transformers-4.28.0\n"]}]},{"cell_type":"code","source":["### Imports memorability classification ###\n","###########################################\n","\n","from transformers import ViTForImageClassification, ViTFeatureExtractor"],"metadata":{"id":"b7BbhoKLTmrH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Load model ###\n","##################\n","\n","# feature extractor\n","mem_feature_extractor = ViTFeatureExtractor.from_pretrained(os.path.join(model_dir, 'best_memorability_model'))\n","\n","# classes\n","labels_mem = ['high_mem', 'low_mem', 'medium_mem']\n","\n","# model\n","mem_model = ViTForImageClassification.from_pretrained(\n","    os.path.join(model_dir, 'best_memorability_model'),\n","    num_labels=len(labels_mem),\n","    id2label={str(i): c for i, c in enumerate(labels_mem)},\n","    label2id={c: str(i) for i, c in enumerate(labels_mem)}\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"170VLKZ3UHDh","executionInfo":{"status":"ok","timestamp":1688147774869,"user_tz":-120,"elapsed":5854,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"outputId":"5c3b6b2a-7fae-4329-ddb6-9481b80f45bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["### Function to apply memorability classification to individal frames of the video ###\n","######################################################################################\n","\n","def mem_detection(video_file):\n","\n","   # initialize video capturing object\n","    cap = cv2.VideoCapture(video_file)\n","\n","    # extract fps to set interval between frames to be contidered\n","    fps = int(cap.get(cv2.CAP_PROP_FPS))\n","\n","    # frame interval -> every n = 2 second, a frame is considered in prediction\n","    frame_interval = 2 * fps\n","\n","    # initialize counter and emotion list object\n","    counter = 0\n","    mem_list = []\n","\n","    # loop though video\n","    while True:\n","      ret, frame = cap.read()\n","\n","      counter+=1\n","\n","      if not ret:\n","        break\n","\n","      if counter % frame_interval != 0:\n","        continue\n","\n","\n","      # feature extraction\n","      inp = mem_feature_extractor(frame[:,:,::-1], return_tensors='pt')\n","\n","      # prediction\n","      with torch.no_grad():\n","        logits = mem_model(inp['pixel_values'])['logits']\n","\n","      mem_list.append(logits.softmax(dim = -1)[0].tolist())\n","\n","\n","\n","    # Release the video capture object and close the windows\n","    cap.release()\n","    cv2.destroyAllWindows()\n","\n","    return (np.array(mem_list).mean(0) @ np.array([2,0,1])) / 3"],"metadata":{"id":"BXPcCIe6U7aC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Apply to all videos ###\n","###########################\n","\n","video_id = []\n","mem_video_list = []\n","\n","for video_file in tqdm(os.listdir('./Video')):\n","\n","  video_id.append(video_file[:-4])\n","  mem_video_list.append(mem_detection(os.path.join('./Video', video_file)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x4BmY_4SVIny","executionInfo":{"status":"ok","timestamp":1688148024977,"user_tz":-120,"elapsed":250123,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"outputId":"0744a6f9-87ff-41ed-8634-aa150bbe7025"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [04:09<00:00,  6.24s/it]\n"]}]},{"cell_type":"code","source":["### Create final dataframe for intent detection ###\n","###################################################\n","\n","mem_df = pd.DataFrame({'mem_score' : mem_video_list})\n","mem_df['video_id'] = video_id"],"metadata":{"id":"W1e5m6mKVKP4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Audio mood classification"],"metadata":{"id":"LHowgHuQYg-u"}},{"cell_type":"code","source":["### Imports audio mood classification ###\n","#########################################\n","\n","import librosa\n","from transformers import AutoFeatureExtractor, AutoModelForAudioClassification"],"metadata":{"id":"ftW_qDFVbkyB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Load model ###\n","##################\n","\n","# audio mood classes\n","audio_mood_classes = ['Q1', 'Q2', 'Q3', 'Q4']\n","\n","# Load feature extractor\n","sound_feature_extractor = AutoFeatureExtractor.from_pretrained(os.path.join(model_dir, 'best_audio_mood_model'))\n","\n","# Load model\n","sound_model = AutoModelForAudioClassification.from_pretrained(os.path.join(model_dir, 'best_audio_mood_model'))"],"metadata":{"id":"CrWrOYhfbzqa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Function to apply audio mood detection to audio files ###\n","#############################################################\n","\n","def audio_mood_detection(audio_file):\n","\n","  # load audio data\n","  audio_data, sr = librosa.load(audio_file)\n","\n","  # resample to sampling rate the model was trained on\n","  a_rs = librosa.resample(audio_data, orig_sr = sr, target_sr = 16000)\n","\n","  # split audio file into 10 parts of equal length\n","  parts = np.linspace(0, len(a_rs), 10)\n","\n","  # loop to consider each part individually\n","  audio_list = []\n","\n","  for i in [0, 3, 5, 7]:\n","\n","    # apply Short-time Fourier Transform to respective part to get model input\n","    inp = sound_feature_extractor(a_rs[int(parts[i]): int(parts[i+1])], sampling_rate=16000, return_tensors=\"pt\")\n","\n","    # extract class probabilities form output logits\n","    with torch.no_grad():\n","      class_probs = sound_model(**inp).logits.softmax(dim = -1)[0]\n","\n","    audio_list.append(class_probs.tolist())\n","\n","  # return mean class probabilites over all audio parts\n","  return list(np.array(audio_list).mean(0))"],"metadata":{"id":"xQO8FExhdi1l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Apply sound detection to all audio files ###\n","################################################\n","\n","video_id = []\n","audio_mood_video_list = []\n","\n","for audio_file in tqdm(os.listdir('./Audio')):\n","\n","  video_id.append(audio_file[:-4])\n","  audio_mood_video_list.append(audio_mood_detection(os.path.join('./Audio', audio_file)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gMTtVT-Vd4rx","executionInfo":{"status":"ok","timestamp":1688148484903,"user_tz":-120,"elapsed":452870,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"outputId":"67fb6137-5e3d-400f-99da-3f63b5eef927"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [07:32<00:00, 11.32s/it]\n"]}]},{"cell_type":"code","source":["### Create final dataframe for sound detection ###\n","##################################################\n","\n","# create df\n","audio_mood_df = pd.DataFrame(audio_mood_video_list)\n","\n","# create dict of col names\n","audio_mood_name_dict = {i:f\"p_audio_mood_{c}\" for i,c in enumerate(audio_mood_classes)}\n","\n","# add names and video_id column\n","audio_mood_df = audio_mood_df.rename(columns = audio_mood_name_dict)\n","audio_mood_df['video_id'] = video_id"],"metadata":{"id":"RbugEcXOeFOl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Merge all 3 dataframes into one ###\n","#######################################\n","\n","# merge\n","high_level_features = intent_df.merge(mem_df, on='video_id').merge(audio_mood_df, on='video_id')\n","\n","# move video_id column to beginning\n","first_column = high_level_features.pop('video_id')\n","high_level_features.insert(0, 'video_id', first_column)"],"metadata":{"id":"4xKrfLUGkM_t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Save as csv file ###\n","########################\n","save_dir = '/content/drive/MyDrive/0_Masterarbeit/2_Pipelines/Feature_outputs'\n","\n","high_level_features.to_csv(f'./high_level_features_{data_file}.csv')\n","shutil.copy(f'./high_level_features_{data_file}.csv', save_dir)"],"metadata":{"id":"UqmmXUVJkmM8","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1688148484908,"user_tz":-120,"elapsed":42,"user":{"displayName":"Sebastian Maier","userId":"16903936560471934380"}},"outputId":"6bc57019-8fb8-4953-bdf0-0076910288a2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/0_Masterarbeit/2_Pipelines/Feature_outputs/high_level_features_nike_part2.csv'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]}]}